# streaming-etl-pipeline
Este script extrae logs crudos de un sistema de archivos, los transforma usando PySpark para limpieza y an谩lisis,y carga los resultados en un Data Lake (Parquet) y una base NoSQL (MongoDB).
#  Streaming Data Pipeline ETL

## Descripci贸n
Este proyecto implementa un pipeline de Ingenier铆a de Datos "End-to-End" para una plataforma de streaming. El sistema ingesta logs de usuarios, procesa la informaci贸n utilizando **Apache Spark** para limpieza y control de calidad, y distribuye los resultados a dos destinos:
1. **Data Lake (Parquet):** Para almacenamiento hist贸rico y auditor铆a.
2. **MongoDB (NoSQL):** Para alimentar un dashboard de "Top Contenidos" en tiempo real.

##  Tecnolog铆as Utilizadas
* **Lenguaje:** Python 3.9
* **Procesamiento:** PySpark (Spark SQL & DataFrames)
* **Infraestructura:** Databricks Community Edition
* **Base de Datos NoSQL:** MongoDB Atlas
* **Orquestaci贸n:** Databricks Workflows

##  Arquitectura del Flujo
1. **Extract:** Generaci贸n/Ingesta de datos crudos (logs de visualizaci贸n).
2. **Transform:** Filtrado de datos corruptos (Nulls, tiempos err贸neos) y agregaci贸n por contenido.
3. **Load:** Carga h铆brida hacia Data Lake (Anal铆tica) y MongoDB (Operacional).

##  Autor
**[Irving Carmona ]** *Ingeniero de Datos en formaci贸n*  [ing.carmona.irving@gmail.com]  
 [Link a tu LinkedIn si tienes]
